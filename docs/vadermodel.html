<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>vader.vadermodel API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>vader.vadermodel</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tensorflow as tf
import numpy as np

class ImputationLayer(tf.keras.layers.Layer):
    def __init__(self, A_init):
        super(ImputationLayer, self).__init__()
        self.A = self.add_weight(
            &#34;A&#34;, shape=A_init.shape, initializer=tf.constant_initializer(A_init))
    def call(self, X, W):
        W = tf.cast(W, X.dtype)
        return X * W + self.A * (1.0 - W)

class RnnDecodeTransformLayer(tf.keras.layers.Layer):
    def __init__(self, n_hidden, I):
        super(RnnDecodeTransformLayer, self).__init__()
        weight_init = tf.constant_initializer(np.random.standard_normal([n_hidden, I]))
        bias_init = tf.constant_initializer(np.zeros(I) + 0.1)
        self.weight = self.add_weight(
            &#34;weight&#34;, shape=[n_hidden, I], initializer=tf.initializers.glorot_uniform())
        self.bias = self.add_weight(
            &#34;bias&#34;, shape=[I], initializer=tf.initializers.glorot_uniform())
    def call(self, rnn_output, batch_size):
        # rnn_output = tf.transpose(rnn_output, perm=[1, 0, 2])
        # rnn_output = tf.transpose(a=tf.stack(rnn_output), perm=[1, 0, 2])
        weight = tf.tile(tf.expand_dims(self.weight, 0), [batch_size, 1, 1])
        return tf.matmul(rnn_output, weight) + self.bias

class GmmLayer(tf.keras.layers.Layer):
    def __init__(self, n_hidden, K):
        super(GmmLayer, self).__init__()
        self.mu_c_unscaled = self.add_weight(
            &#34;mu_c_unscaled&#34;, shape=[K, n_hidden], initializer=tf.initializers.glorot_uniform())
        self.sigma2_c_unscaled = self.add_weight(
            &#34;sigma2_c_unscaled&#34;, shape=[K, n_hidden], initializer=tf.initializers.glorot_uniform())
        self.phi_c_unscaled = self.add_weight(
            &#34;phi_c_unscaled&#34;, shape=[K], initializer=tf.initializers.glorot_uniform())
    def call(self, _):
        mu_c = self.mu_c_unscaled
        sigma2_c = tf.nn.softplus(self.sigma2_c_unscaled, name=&#34;sigma2_c&#34;)
        phi_c = tf.nn.softmax(self.phi_c_unscaled, name=&#34;phi_c&#34;)
        return mu_c, sigma2_c, phi_c

class VaderModel(tf.keras.Model):
    def __init__(self, X, W, D, K, I, cell_type, n_hidden, recurrent, output_activation):
        super(VaderModel, self).__init__()
        self.D = D
        self.K = K
        self.I = I
        self.cell_type = cell_type
        self.n_hidden = n_hidden
        self.recurrent = recurrent
        self.output_activation = output_activation
        def initialize_imputation(X, W):
            # average per time point, variable
            W_A = np.sum(W, axis=0)
            A = np.sum(X * W, axis=0)
            A[W_A &gt; 0] = A[W_A &gt; 0] / W_A[W_A &gt; 0]
            # if not available, then average across entire variable
            if recurrent:
                for i in np.arange(A.shape[0]):
                    for j in np.arange(A.shape[1]):
                        if W_A[i, j] == 0:
                            A[i, j] = np.sum(X[:, :, j]) / np.sum(W[:, :, j])
                            W_A[i, j] = 1
            # if not available, then average across all variables
            A[W_A == 0] = np.mean(X[W == 1])
            return A.astype(np.float32)

        def sample(params):
            mu_tilde = params[0]
            log_sigma2_tilde = params[1]
            noise = tf.random.normal(tf.shape(input=log_sigma2_tilde))
            return tf.add(mu_tilde, tf.exp(0.5 * log_sigma2_tilde) * noise, name=&#34;z&#34;)

        self.imputation_layer = ImputationLayer(initialize_imputation(X, W))
        self.gmm_layer = GmmLayer(n_hidden[-1], K)
        self.z_layer = tf.keras.layers.Lambda(sample, name=&#34;z&#34;)
        if recurrent:
            if len(n_hidden) &gt; 1:
                if cell_type == &#34;LSTM&#34;:
                    encoder = tf.keras.experimental.PeepholeLSTMCell(n_hidden[0], activation=tf.nn.tanh, name=&#34;encoder&#34;)
                    decoder = tf.keras.experimental.PeepholeLSTMCell(n_hidden[0], activation=tf.nn.tanh, name=&#34;decoder&#34;)
                else:
                    encoder = tf.keras.layers.GRUCell(n_hidden[0], activation=tf.nn.tanh, name=&#34;encoder&#34;)
                    decoder = tf.keras.layers.GRUCell(n_hidden[0], activation=tf.nn.tanh, name=&#34;decoder&#34;)
                self.ae_encode_layers = [tf.keras.layers.Dense(n, activation=tf.nn.softplus) for n in n_hidden[1:-1]]
                self.mu_layer = tf.keras.layers.Dense(n_hidden[-1], activation=None, name=&#34;mu_tilde&#34;)
                self.log_sigma2_layer = tf.keras.layers.Dense(n_hidden[-1], activation=None, name=&#34;log_sigma2_tilde&#34;)
                self.rnn_transform_layer = RnnDecodeTransformLayer(n_hidden[0], I)
                self.ae_decode_layers = [tf.keras.layers.Dense(n, activation=tf.nn.softplus) for n in (n_hidden[:-1])[::-1]]
            else:
                n_hidden = n_hidden[0]
                if cell_type == &#34;LSTM&#34;:
                    # one n_hidden for mu, one for sigma2
                    encoder = tf.keras.experimental.PeepholeLSTMCell(
                        n_hidden + n_hidden, activation=tf.nn.tanh, name=&#34;encoder&#34;)
                    decoder = tf.keras.experimental.PeepholeLSTMCell(n_hidden, activation=tf.nn.tanh, name=&#34;decoder&#34;)
                else:
                    # one n_hidden for mu, one for sigma2
                    encoder = tf.keras.layers.GRUCell(n_hidden + n_hidden, activation=tf.nn.tanh, name=&#34;encoder&#34;)
                    decoder = tf.keras.layers.GRUCell(n_hidden, activation=tf.nn.tanh, name=&#34;decoder&#34;)
                self.mu_layer = tf.keras.layers.Lambda(lambda hidden: hidden[:, :n_hidden], name=&#34;mu_tilde&#34;)
                self.log_sigma2_layer = tf.keras.layers.Lambda(
                    lambda hidden: hidden[:, n_hidden:], name=&#34;log_sigma2_tilde&#34;)
                self.rnn_transform_layer = RnnDecodeTransformLayer(n_hidden, I)
            # select the carry state (not the memory state)
            # unroll = True raises &#34;ValueError: Cannot unroll a RNN if the time dimension is undefined.&#34;
            self.encoder_rnn = tf.keras.layers.RNN(encoder, unroll=False, return_state=True, return_sequences=True)
            # select the output
            self.decoder_rnn = tf.keras.layers.RNN(decoder, unroll=False, return_state=True, return_sequences=True)
        else:
            self.ae_encode_layers = [tf.keras.layers.Dense(n, activation=tf.nn.softplus) for n in n_hidden[:-1]]
            self.mu_layer = tf.keras.layers.Dense(n_hidden[-1], activation=None, name=&#34;mu_tilde&#34;)
            self.log_sigma2_layer = tf.keras.layers.Dense(n_hidden[-1], activation=None, name=&#34;log_sigma2_tilde&#34;)
            self.ae_decode_layers = [tf.keras.layers.Dense(n, activation=tf.nn.softplus) for n in (n_hidden[:-1])[::-1]]
            self.x_raw_layer = tf.keras.layers.Dense(D, activation=None, name=&#34;x_raw&#34;)
    @tf.function
    def encode(self, X, W):
        X_imputed = self.imputation_layer(X, W)
        if self.recurrent:
            # X_imputed = [tf.squeeze(t, [1]) for t in tf.split(X_imputed, self.D, 1)]
            hidden = self.encoder_rnn(X_imputed)[-1] # [1] ???
        else:
            hidden = X_imputed
        if len(self.n_hidden) &gt; 1:
            for layer in self.ae_encode_layers:
                hidden = layer(hidden)
        mu_tilde = self.mu_layer(hidden)
        log_sigma2_tilde = self.log_sigma2_layer(hidden)
        z = self.z_layer((mu_tilde, log_sigma2_tilde))
        return z, mu_tilde, log_sigma2_tilde

    @tf.function
    def decode(self, z):
        hidden = z
        if len(self.n_hidden) &gt; 1:
            for layer in self.ae_decode_layers:
                hidden = layer(hidden)
        if self.recurrent:
            inputs = tf.zeros((tf.shape(z)[0], self.D, self.I))
            initial_state = (tf.zeros(tf.shape(input=hidden)), hidden)
            rnn_output = self.decoder_rnn(inputs=inputs, initial_state=initial_state)[0]
            x_raw = self.rnn_transform_layer(rnn_output, tf.shape(z)[0])
        else:
            x_raw = self.x_raw_layer(hidden)
        x = self.output_activation(x_raw, name=&#34;x_output&#34;)
        return x, x_raw

    @tf.function
    def call(self, inputs, training=False):
        X = inputs[0]
        W = inputs[1]
        z, mu_tilde, log_sigma2_tilde = self.encode(X, W)
        x, x_raw = self.decode(z)
        dummy_val = tf.constant(0.0, dtype=tf.float32)
        mu_c, sigma2_c, phi_c = self.gmm_layer(dummy_val)
        return x, x_raw, mu_c, sigma2_c, phi_c, z, mu_tilde, log_sigma2_tilde</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="vader.vadermodel.GmmLayer"><code class="flex name class">
<span>class <span class="ident">GmmLayer</span></span>
<span>(</span><span>n_hidden, K)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables), defined
either in the constructor <code>__init__()</code> or in the <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<h2 id="arguments">Arguments</h2>
<p>trainable: Boolean, whether the layer's variables should be trainable.
name: String name of the layer.
dtype: The dtype of the layer's computations and weights (default of
<code>None</code> means use <code>tf.keras.backend.floatx</code> in TensorFlow 2, or the type
of the first input in TensorFlow 1).
dynamic: Set this to <code>True</code> if your layer should only be run eagerly, and
should not be used to generate a static computation graph.
This would be the case for a Tree-RNN or a recursive network,
for example, or generally for any layer that manipulates tensors
using Python control flow. If <code>False</code>, we assume that the layer can
safely be used to generate a static computation graph.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>The name of the layer (string).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dtype of the layer's computations and weights. If mixed
precision is used with a <code>tf.keras.mixed_precision.experimental.Policy</code>,
this is instead just the dtype of the layer's weights, as the computations
are done in a different dtype.</dd>
<dt><strong><code>trainable_weights</code></strong></dt>
<dd>List of variables to be included in backprop.</dd>
<dt><strong><code>non_trainable_weights</code></strong></dt>
<dd>List of variables that should not be
included in backprop.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>The concatenation of the lists trainable_weights and
non_trainable_weights (in this order).</dd>
<dt><strong><code>trainable</code></strong></dt>
<dd>Whether the layer should be trained (boolean), i.e. whether
its potentially-trainable weights should be returned as part of
<code>layer.trainable_weights</code>.</dd>
<dt><strong><code>input_spec</code></strong></dt>
<dd>Optional (list of) <code>InputSpec</code> object(s) specifying the
constraints on inputs that can be accepted by the layer.</dd>
</dl>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer state
variables that do not depend on input shapes, using <code>add_weight()</code>.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>. <code>__call__()</code>
will automatically build the layer (if it has not been built yet) by
calling <code>build()</code>.</li>
<li><code>call(self, *args, **kwargs)</code>: Called in <code>__call__</code> after making sure
<code>build()</code> has been called. <code>call()</code> performs the logic of applying the
layer to the input tensors (which should be passed in as argument).
Two reserved keyword arguments you can optionally use in <code>call()</code> are:<ul>
<li><code>training</code> (boolean, whether the call is in
inference mode or training mode)</li>
<li><code>mask</code> (boolean tensor encoding masked timesteps in the input, used
in RNN layers)</li>
</ul>
</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration used
to initialize this layer. If the keys differ from the arguments
in <code>__init__</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
</code></pre>
<p>Note that the method <code>add_weight()</code> offers a shortcut to create weights:</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b
</code></pre>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<pre><code class="language-python">class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
</code></pre>
<p>For more information about creating layers, see the guide
<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Writing custom layers and models with Keras</a></p>
<p>About the layer's <code>dtype</code> attribute:</p>
<p>Each layer has a dtype, which is typically the dtype of the layer's
computations and variables. A layer's dtype can be queried via the
<code>Layer.dtype</code> property. The dtype is specified with the <code>dtype</code> constructor
argument. In TensorFlow 2, the dtype defaults to <code>tf.keras.backend.floatx()</code>
if no dtype is passed. <code>floatx()</code> itself defaults to "float32". Additionally,
layers will cast their inputs to the layer's dtype in TensorFlow 2. When mixed
precision is used, layers may have different computation and variable dtypes.
See <code>tf.keras.mixed_precision.experimental.Policy</code> for details on layer
dtypes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GmmLayer(tf.keras.layers.Layer):
    def __init__(self, n_hidden, K):
        super(GmmLayer, self).__init__()
        self.mu_c_unscaled = self.add_weight(
            &#34;mu_c_unscaled&#34;, shape=[K, n_hidden], initializer=tf.initializers.glorot_uniform())
        self.sigma2_c_unscaled = self.add_weight(
            &#34;sigma2_c_unscaled&#34;, shape=[K, n_hidden], initializer=tf.initializers.glorot_uniform())
        self.phi_c_unscaled = self.add_weight(
            &#34;phi_c_unscaled&#34;, shape=[K], initializer=tf.initializers.glorot_uniform())
    def call(self, _):
        mu_c = self.mu_c_unscaled
        sigma2_c = tf.nn.softplus(self.sigma2_c_unscaled, name=&#34;sigma2_c&#34;)
        phi_c = tf.nn.softmax(self.phi_c_unscaled, name=&#34;phi_c&#34;)
        return mu_c, sigma2_c, phi_c</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vader.vadermodel.GmmLayer.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, _)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: Input tensor, or list/tuple of input tensors.
**kwargs: Additional keyword arguments. Currently unused.</p>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, _):
    mu_c = self.mu_c_unscaled
    sigma2_c = tf.nn.softplus(self.sigma2_c_unscaled, name=&#34;sigma2_c&#34;)
    phi_c = tf.nn.softmax(self.phi_c_unscaled, name=&#34;phi_c&#34;)
    return mu_c, sigma2_c, phi_c</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="vader.vadermodel.ImputationLayer"><code class="flex name class">
<span>class <span class="ident">ImputationLayer</span></span>
<span>(</span><span>A_init)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables), defined
either in the constructor <code>__init__()</code> or in the <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<h2 id="arguments">Arguments</h2>
<p>trainable: Boolean, whether the layer's variables should be trainable.
name: String name of the layer.
dtype: The dtype of the layer's computations and weights (default of
<code>None</code> means use <code>tf.keras.backend.floatx</code> in TensorFlow 2, or the type
of the first input in TensorFlow 1).
dynamic: Set this to <code>True</code> if your layer should only be run eagerly, and
should not be used to generate a static computation graph.
This would be the case for a Tree-RNN or a recursive network,
for example, or generally for any layer that manipulates tensors
using Python control flow. If <code>False</code>, we assume that the layer can
safely be used to generate a static computation graph.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>The name of the layer (string).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dtype of the layer's computations and weights. If mixed
precision is used with a <code>tf.keras.mixed_precision.experimental.Policy</code>,
this is instead just the dtype of the layer's weights, as the computations
are done in a different dtype.</dd>
<dt><strong><code>trainable_weights</code></strong></dt>
<dd>List of variables to be included in backprop.</dd>
<dt><strong><code>non_trainable_weights</code></strong></dt>
<dd>List of variables that should not be
included in backprop.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>The concatenation of the lists trainable_weights and
non_trainable_weights (in this order).</dd>
<dt><strong><code>trainable</code></strong></dt>
<dd>Whether the layer should be trained (boolean), i.e. whether
its potentially-trainable weights should be returned as part of
<code>layer.trainable_weights</code>.</dd>
<dt><strong><code>input_spec</code></strong></dt>
<dd>Optional (list of) <code>InputSpec</code> object(s) specifying the
constraints on inputs that can be accepted by the layer.</dd>
</dl>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer state
variables that do not depend on input shapes, using <code>add_weight()</code>.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>. <code>__call__()</code>
will automatically build the layer (if it has not been built yet) by
calling <code>build()</code>.</li>
<li><code>call(self, *args, **kwargs)</code>: Called in <code>__call__</code> after making sure
<code>build()</code> has been called. <code>call()</code> performs the logic of applying the
layer to the input tensors (which should be passed in as argument).
Two reserved keyword arguments you can optionally use in <code>call()</code> are:<ul>
<li><code>training</code> (boolean, whether the call is in
inference mode or training mode)</li>
<li><code>mask</code> (boolean tensor encoding masked timesteps in the input, used
in RNN layers)</li>
</ul>
</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration used
to initialize this layer. If the keys differ from the arguments
in <code>__init__</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
</code></pre>
<p>Note that the method <code>add_weight()</code> offers a shortcut to create weights:</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b
</code></pre>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<pre><code class="language-python">class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
</code></pre>
<p>For more information about creating layers, see the guide
<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Writing custom layers and models with Keras</a></p>
<p>About the layer's <code>dtype</code> attribute:</p>
<p>Each layer has a dtype, which is typically the dtype of the layer's
computations and variables. A layer's dtype can be queried via the
<code>Layer.dtype</code> property. The dtype is specified with the <code>dtype</code> constructor
argument. In TensorFlow 2, the dtype defaults to <code>tf.keras.backend.floatx()</code>
if no dtype is passed. <code>floatx()</code> itself defaults to "float32". Additionally,
layers will cast their inputs to the layer's dtype in TensorFlow 2. When mixed
precision is used, layers may have different computation and variable dtypes.
See <code>tf.keras.mixed_precision.experimental.Policy</code> for details on layer
dtypes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ImputationLayer(tf.keras.layers.Layer):
    def __init__(self, A_init):
        super(ImputationLayer, self).__init__()
        self.A = self.add_weight(
            &#34;A&#34;, shape=A_init.shape, initializer=tf.constant_initializer(A_init))
    def call(self, X, W):
        W = tf.cast(W, X.dtype)
        return X * W + self.A * (1.0 - W)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vader.vadermodel.ImputationLayer.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, X, W)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: Input tensor, or list/tuple of input tensors.
**kwargs: Additional keyword arguments. Currently unused.</p>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, X, W):
    W = tf.cast(W, X.dtype)
    return X * W + self.A * (1.0 - W)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="vader.vadermodel.RnnDecodeTransformLayer"><code class="flex name class">
<span>class <span class="ident">RnnDecodeTransformLayer</span></span>
<span>(</span><span>n_hidden, I)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables), defined
either in the constructor <code>__init__()</code> or in the <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<h2 id="arguments">Arguments</h2>
<p>trainable: Boolean, whether the layer's variables should be trainable.
name: String name of the layer.
dtype: The dtype of the layer's computations and weights (default of
<code>None</code> means use <code>tf.keras.backend.floatx</code> in TensorFlow 2, or the type
of the first input in TensorFlow 1).
dynamic: Set this to <code>True</code> if your layer should only be run eagerly, and
should not be used to generate a static computation graph.
This would be the case for a Tree-RNN or a recursive network,
for example, or generally for any layer that manipulates tensors
using Python control flow. If <code>False</code>, we assume that the layer can
safely be used to generate a static computation graph.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>The name of the layer (string).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dtype of the layer's computations and weights. If mixed
precision is used with a <code>tf.keras.mixed_precision.experimental.Policy</code>,
this is instead just the dtype of the layer's weights, as the computations
are done in a different dtype.</dd>
<dt><strong><code>trainable_weights</code></strong></dt>
<dd>List of variables to be included in backprop.</dd>
<dt><strong><code>non_trainable_weights</code></strong></dt>
<dd>List of variables that should not be
included in backprop.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>The concatenation of the lists trainable_weights and
non_trainable_weights (in this order).</dd>
<dt><strong><code>trainable</code></strong></dt>
<dd>Whether the layer should be trained (boolean), i.e. whether
its potentially-trainable weights should be returned as part of
<code>layer.trainable_weights</code>.</dd>
<dt><strong><code>input_spec</code></strong></dt>
<dd>Optional (list of) <code>InputSpec</code> object(s) specifying the
constraints on inputs that can be accepted by the layer.</dd>
</dl>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer state
variables that do not depend on input shapes, using <code>add_weight()</code>.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>. <code>__call__()</code>
will automatically build the layer (if it has not been built yet) by
calling <code>build()</code>.</li>
<li><code>call(self, *args, **kwargs)</code>: Called in <code>__call__</code> after making sure
<code>build()</code> has been called. <code>call()</code> performs the logic of applying the
layer to the input tensors (which should be passed in as argument).
Two reserved keyword arguments you can optionally use in <code>call()</code> are:<ul>
<li><code>training</code> (boolean, whether the call is in
inference mode or training mode)</li>
<li><code>mask</code> (boolean tensor encoding masked timesteps in the input, used
in RNN layers)</li>
</ul>
</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration used
to initialize this layer. If the keys differ from the arguments
in <code>__init__</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
</code></pre>
<p>Note that the method <code>add_weight()</code> offers a shortcut to create weights:</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b
</code></pre>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<pre><code class="language-python">class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
</code></pre>
<p>For more information about creating layers, see the guide
<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Writing custom layers and models with Keras</a></p>
<p>About the layer's <code>dtype</code> attribute:</p>
<p>Each layer has a dtype, which is typically the dtype of the layer's
computations and variables. A layer's dtype can be queried via the
<code>Layer.dtype</code> property. The dtype is specified with the <code>dtype</code> constructor
argument. In TensorFlow 2, the dtype defaults to <code>tf.keras.backend.floatx()</code>
if no dtype is passed. <code>floatx()</code> itself defaults to "float32". Additionally,
layers will cast their inputs to the layer's dtype in TensorFlow 2. When mixed
precision is used, layers may have different computation and variable dtypes.
See <code>tf.keras.mixed_precision.experimental.Policy</code> for details on layer
dtypes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RnnDecodeTransformLayer(tf.keras.layers.Layer):
    def __init__(self, n_hidden, I):
        super(RnnDecodeTransformLayer, self).__init__()
        weight_init = tf.constant_initializer(np.random.standard_normal([n_hidden, I]))
        bias_init = tf.constant_initializer(np.zeros(I) + 0.1)
        self.weight = self.add_weight(
            &#34;weight&#34;, shape=[n_hidden, I], initializer=tf.initializers.glorot_uniform())
        self.bias = self.add_weight(
            &#34;bias&#34;, shape=[I], initializer=tf.initializers.glorot_uniform())
    def call(self, rnn_output, batch_size):
        # rnn_output = tf.transpose(rnn_output, perm=[1, 0, 2])
        # rnn_output = tf.transpose(a=tf.stack(rnn_output), perm=[1, 0, 2])
        weight = tf.tile(tf.expand_dims(self.weight, 0), [batch_size, 1, 1])
        return tf.matmul(rnn_output, weight) + self.bias</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vader.vadermodel.RnnDecodeTransformLayer.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, rnn_output, batch_size)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: Input tensor, or list/tuple of input tensors.
**kwargs: Additional keyword arguments. Currently unused.</p>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, rnn_output, batch_size):
    # rnn_output = tf.transpose(rnn_output, perm=[1, 0, 2])
    # rnn_output = tf.transpose(a=tf.stack(rnn_output), perm=[1, 0, 2])
    weight = tf.tile(tf.expand_dims(self.weight, 0), [batch_size, 1, 1])
    return tf.matmul(rnn_output, weight) + self.bias</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="vader.vadermodel.VaderModel"><code class="flex name class">
<span>class <span class="ident">VaderModel</span></span>
<span>(</span><span>X, W, D, K, I, cell_type, n_hidden, recurrent, output_activation)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="language-python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="language-python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="language-python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VaderModel(tf.keras.Model):
    def __init__(self, X, W, D, K, I, cell_type, n_hidden, recurrent, output_activation):
        super(VaderModel, self).__init__()
        self.D = D
        self.K = K
        self.I = I
        self.cell_type = cell_type
        self.n_hidden = n_hidden
        self.recurrent = recurrent
        self.output_activation = output_activation
        def initialize_imputation(X, W):
            # average per time point, variable
            W_A = np.sum(W, axis=0)
            A = np.sum(X * W, axis=0)
            A[W_A &gt; 0] = A[W_A &gt; 0] / W_A[W_A &gt; 0]
            # if not available, then average across entire variable
            if recurrent:
                for i in np.arange(A.shape[0]):
                    for j in np.arange(A.shape[1]):
                        if W_A[i, j] == 0:
                            A[i, j] = np.sum(X[:, :, j]) / np.sum(W[:, :, j])
                            W_A[i, j] = 1
            # if not available, then average across all variables
            A[W_A == 0] = np.mean(X[W == 1])
            return A.astype(np.float32)

        def sample(params):
            mu_tilde = params[0]
            log_sigma2_tilde = params[1]
            noise = tf.random.normal(tf.shape(input=log_sigma2_tilde))
            return tf.add(mu_tilde, tf.exp(0.5 * log_sigma2_tilde) * noise, name=&#34;z&#34;)

        self.imputation_layer = ImputationLayer(initialize_imputation(X, W))
        self.gmm_layer = GmmLayer(n_hidden[-1], K)
        self.z_layer = tf.keras.layers.Lambda(sample, name=&#34;z&#34;)
        if recurrent:
            if len(n_hidden) &gt; 1:
                if cell_type == &#34;LSTM&#34;:
                    encoder = tf.keras.experimental.PeepholeLSTMCell(n_hidden[0], activation=tf.nn.tanh, name=&#34;encoder&#34;)
                    decoder = tf.keras.experimental.PeepholeLSTMCell(n_hidden[0], activation=tf.nn.tanh, name=&#34;decoder&#34;)
                else:
                    encoder = tf.keras.layers.GRUCell(n_hidden[0], activation=tf.nn.tanh, name=&#34;encoder&#34;)
                    decoder = tf.keras.layers.GRUCell(n_hidden[0], activation=tf.nn.tanh, name=&#34;decoder&#34;)
                self.ae_encode_layers = [tf.keras.layers.Dense(n, activation=tf.nn.softplus) for n in n_hidden[1:-1]]
                self.mu_layer = tf.keras.layers.Dense(n_hidden[-1], activation=None, name=&#34;mu_tilde&#34;)
                self.log_sigma2_layer = tf.keras.layers.Dense(n_hidden[-1], activation=None, name=&#34;log_sigma2_tilde&#34;)
                self.rnn_transform_layer = RnnDecodeTransformLayer(n_hidden[0], I)
                self.ae_decode_layers = [tf.keras.layers.Dense(n, activation=tf.nn.softplus) for n in (n_hidden[:-1])[::-1]]
            else:
                n_hidden = n_hidden[0]
                if cell_type == &#34;LSTM&#34;:
                    # one n_hidden for mu, one for sigma2
                    encoder = tf.keras.experimental.PeepholeLSTMCell(
                        n_hidden + n_hidden, activation=tf.nn.tanh, name=&#34;encoder&#34;)
                    decoder = tf.keras.experimental.PeepholeLSTMCell(n_hidden, activation=tf.nn.tanh, name=&#34;decoder&#34;)
                else:
                    # one n_hidden for mu, one for sigma2
                    encoder = tf.keras.layers.GRUCell(n_hidden + n_hidden, activation=tf.nn.tanh, name=&#34;encoder&#34;)
                    decoder = tf.keras.layers.GRUCell(n_hidden, activation=tf.nn.tanh, name=&#34;decoder&#34;)
                self.mu_layer = tf.keras.layers.Lambda(lambda hidden: hidden[:, :n_hidden], name=&#34;mu_tilde&#34;)
                self.log_sigma2_layer = tf.keras.layers.Lambda(
                    lambda hidden: hidden[:, n_hidden:], name=&#34;log_sigma2_tilde&#34;)
                self.rnn_transform_layer = RnnDecodeTransformLayer(n_hidden, I)
            # select the carry state (not the memory state)
            # unroll = True raises &#34;ValueError: Cannot unroll a RNN if the time dimension is undefined.&#34;
            self.encoder_rnn = tf.keras.layers.RNN(encoder, unroll=False, return_state=True, return_sequences=True)
            # select the output
            self.decoder_rnn = tf.keras.layers.RNN(decoder, unroll=False, return_state=True, return_sequences=True)
        else:
            self.ae_encode_layers = [tf.keras.layers.Dense(n, activation=tf.nn.softplus) for n in n_hidden[:-1]]
            self.mu_layer = tf.keras.layers.Dense(n_hidden[-1], activation=None, name=&#34;mu_tilde&#34;)
            self.log_sigma2_layer = tf.keras.layers.Dense(n_hidden[-1], activation=None, name=&#34;log_sigma2_tilde&#34;)
            self.ae_decode_layers = [tf.keras.layers.Dense(n, activation=tf.nn.softplus) for n in (n_hidden[:-1])[::-1]]
            self.x_raw_layer = tf.keras.layers.Dense(D, activation=None, name=&#34;x_raw&#34;)
    @tf.function
    def encode(self, X, W):
        X_imputed = self.imputation_layer(X, W)
        if self.recurrent:
            # X_imputed = [tf.squeeze(t, [1]) for t in tf.split(X_imputed, self.D, 1)]
            hidden = self.encoder_rnn(X_imputed)[-1] # [1] ???
        else:
            hidden = X_imputed
        if len(self.n_hidden) &gt; 1:
            for layer in self.ae_encode_layers:
                hidden = layer(hidden)
        mu_tilde = self.mu_layer(hidden)
        log_sigma2_tilde = self.log_sigma2_layer(hidden)
        z = self.z_layer((mu_tilde, log_sigma2_tilde))
        return z, mu_tilde, log_sigma2_tilde

    @tf.function
    def decode(self, z):
        hidden = z
        if len(self.n_hidden) &gt; 1:
            for layer in self.ae_decode_layers:
                hidden = layer(hidden)
        if self.recurrent:
            inputs = tf.zeros((tf.shape(z)[0], self.D, self.I))
            initial_state = (tf.zeros(tf.shape(input=hidden)), hidden)
            rnn_output = self.decoder_rnn(inputs=inputs, initial_state=initial_state)[0]
            x_raw = self.rnn_transform_layer(rnn_output, tf.shape(z)[0])
        else:
            x_raw = self.x_raw_layer(hidden)
        x = self.output_activation(x_raw, name=&#34;x_output&#34;)
        return x, x_raw

    @tf.function
    def call(self, inputs, training=False):
        X = inputs[0]
        W = inputs[1]
        z, mu_tilde, log_sigma2_tilde = self.encode(X, W)
        x, x_raw = self.decode(z)
        dummy_val = tf.constant(0.0, dtype=tf.float32)
        mu_c, sigma2_c, phi_c = self.gmm_layer(dummy_val)
        return x, x_raw, mu_c, sigma2_c, phi_c, z, mu_tilde, log_sigma2_tilde</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vader.vadermodel.VaderModel.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@tf.function
def call(self, inputs, training=False):
    X = inputs[0]
    W = inputs[1]
    z, mu_tilde, log_sigma2_tilde = self.encode(X, W)
    x, x_raw = self.decode(z)
    dummy_val = tf.constant(0.0, dtype=tf.float32)
    mu_c, sigma2_c, phi_c = self.gmm_layer(dummy_val)
    return x, x_raw, mu_c, sigma2_c, phi_c, z, mu_tilde, log_sigma2_tilde</code></pre>
</details>
</dd>
<dt id="vader.vadermodel.VaderModel.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, z)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@tf.function
def decode(self, z):
    hidden = z
    if len(self.n_hidden) &gt; 1:
        for layer in self.ae_decode_layers:
            hidden = layer(hidden)
    if self.recurrent:
        inputs = tf.zeros((tf.shape(z)[0], self.D, self.I))
        initial_state = (tf.zeros(tf.shape(input=hidden)), hidden)
        rnn_output = self.decoder_rnn(inputs=inputs, initial_state=initial_state)[0]
        x_raw = self.rnn_transform_layer(rnn_output, tf.shape(z)[0])
    else:
        x_raw = self.x_raw_layer(hidden)
    x = self.output_activation(x_raw, name=&#34;x_output&#34;)
    return x, x_raw</code></pre>
</details>
</dd>
<dt id="vader.vadermodel.VaderModel.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, X, W)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@tf.function
def encode(self, X, W):
    X_imputed = self.imputation_layer(X, W)
    if self.recurrent:
        # X_imputed = [tf.squeeze(t, [1]) for t in tf.split(X_imputed, self.D, 1)]
        hidden = self.encoder_rnn(X_imputed)[-1] # [1] ???
    else:
        hidden = X_imputed
    if len(self.n_hidden) &gt; 1:
        for layer in self.ae_encode_layers:
            hidden = layer(hidden)
    mu_tilde = self.mu_layer(hidden)
    log_sigma2_tilde = self.log_sigma2_layer(hidden)
    z = self.z_layer((mu_tilde, log_sigma2_tilde))
    return z, mu_tilde, log_sigma2_tilde</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="vader" href="index.html">vader</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="vader.vadermodel.GmmLayer" href="#vader.vadermodel.GmmLayer">GmmLayer</a></code></h4>
<ul class="">
<li><code><a title="vader.vadermodel.GmmLayer.call" href="#vader.vadermodel.GmmLayer.call">call</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vader.vadermodel.ImputationLayer" href="#vader.vadermodel.ImputationLayer">ImputationLayer</a></code></h4>
<ul class="">
<li><code><a title="vader.vadermodel.ImputationLayer.call" href="#vader.vadermodel.ImputationLayer.call">call</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vader.vadermodel.RnnDecodeTransformLayer" href="#vader.vadermodel.RnnDecodeTransformLayer">RnnDecodeTransformLayer</a></code></h4>
<ul class="">
<li><code><a title="vader.vadermodel.RnnDecodeTransformLayer.call" href="#vader.vadermodel.RnnDecodeTransformLayer.call">call</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vader.vadermodel.VaderModel" href="#vader.vadermodel.VaderModel">VaderModel</a></code></h4>
<ul class="">
<li><code><a title="vader.vadermodel.VaderModel.call" href="#vader.vadermodel.VaderModel.call">call</a></code></li>
<li><code><a title="vader.vadermodel.VaderModel.decode" href="#vader.vadermodel.VaderModel.decode">decode</a></code></li>
<li><code><a title="vader.vadermodel.VaderModel.encode" href="#vader.vadermodel.VaderModel.encode">encode</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>